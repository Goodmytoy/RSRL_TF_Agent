{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from tf_agents.utils import nest_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "through-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacterial-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "# read dat file\n",
    "ratings_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'ratings.dat'), 'r').readlines()]\n",
    "users_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'users.dat'), 'r').readlines()]\n",
    "movies_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'movies.dat'),encoding='latin-1').readlines()]\n",
    "\n",
    "# Craete DataFrame\n",
    "ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = np.uint32)\n",
    "ratings_df = ratings_df.astype(int).sort_values([\"UserID\", \"Timestamp\"])\n",
    "\n",
    "\n",
    "movies_df = pd.DataFrame(movies_list, columns = ['MovieID', 'Title', 'Genres'])\n",
    "movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)\n",
    "users_df = pd.DataFrame(users_list, columns=['UserID','Gender','Age','Occupation','Zip-code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "premier-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "STATE_SIZE = 10\n",
    "ACTOR_LEARNIG_RATE = 0.001\n",
    "CRITIC_LEARNIG_RATE = 0.001\n",
    "\n",
    "log_interval = 25\n",
    "eval_interval  = 50\n",
    "\n",
    "NUM_EVAL_EPISODES = 10\n",
    "\n",
    "REPLAY_BUFFER_MAX_LENGTH = 50000\n",
    "NUM_EPISODE = 10000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optimum-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ratings_df.groupby(\"UserID\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adaptive-practice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "racial-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp[\"MovieID\"] >= 50).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-times",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fleet-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserMovieEmbedding(tf.keras.Model):\n",
    "    def __init__(self, len_users, len_movies, embedding_dim):\n",
    "        super(UserMovieEmbedding, self).__init__()\n",
    "        self.m_u_input = tf.keras.layers.InputLayer(name='input_layer', input_shape=(2,))\n",
    "        # embedding\n",
    "        self.u_embedding = tf.keras.layers.Embedding(name='user_embedding', input_dim=len_users, output_dim=embedding_dim)\n",
    "        self.m_embedding = tf.keras.layers.Embedding(name='movie_embedding', input_dim=len_movies, output_dim=embedding_dim)\n",
    "        # dot product\n",
    "        self.m_u_merge = tf.keras.layers.Dot(name='movie_user_dot', normalize=False, axes=1)\n",
    "        # output\n",
    "        self.m_u_fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.m_u_input(x)\n",
    "        uemb = self.u_embedding(x[0])\n",
    "        memb = self.m_embedding(x[1])\n",
    "        m_u = self.m_u_merge([memb, uemb])\n",
    "        return self.m_u_fc(m_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "large-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_num = ratings_df[\"UserID\"].max() + 1\n",
    "items_num = ratings_df[\"MovieID\"].max() + 1\n",
    "\n",
    "embedding_network = UserMovieEmbedding(users_num, items_num, EMBEDDING_DIM)\n",
    "embedding_network([np.zeros((1,)),np.zeros((1,))])\n",
    "embedding_network.load_weights('save_weights/user_movie_embedding_case4.h5')\n",
    "\n",
    "items_ids = np.array(range(items_num))\n",
    "movie_embedding = embedding_network.get_layer('movie_embedding')(items_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-seafood",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southern-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRRAveStateRepresentation(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(DRRAveStateRepresentation, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.wav = tf.keras.layers.Conv1D(1, 1, 1)\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    def call(self, x):\n",
    "        items_eb = tf.transpose(x[1], perm=(0,2,1))/self.embedding_dim\n",
    "        wav = self.wav(items_eb)\n",
    "        wav = tf.transpose(wav, perm=(0,2,1))\n",
    "        wav = tf.squeeze(wav, axis=1)\n",
    "        user_wav = tf.keras.layers.multiply([x[0], wav])\n",
    "        concat = self.concat([x[0], user_wav, wav])\n",
    "        return self.flatten(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-tragedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mechanical-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RS_Env(py_environment.PyEnvironment):\n",
    "    def __init__(self, ratings_df, embedding_dim, state_size, embedding_network):\n",
    "        self.users_num = ratings_df[\"UserID\"].max() + 1\n",
    "        self.items_num = ratings_df[\"MovieID\"].max() + 1\n",
    "        self.ratings_df = ratings_df\n",
    "        self.pos_ratings_df = ratings_df.loc[ratings_df[\"Rating\"] >= 4]\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_network = embedding_network\n",
    "        self.state_size = state_size\n",
    "        self.max_step = 1000\n",
    "        \n",
    "            \n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape = (embedding_dim, ), dtype = np.float32, maximum = 1, minimum = -1, name = \"action\")\n",
    "        self._observation_spec = array_spec.ArraySpec(shape = (3*self.embedding_dim, ), dtype = np.float32, name = \"state_representation\")\n",
    "        \n",
    "        \n",
    "        self.valid_users = self._generate_valid_user()\n",
    "        \n",
    "        # reset env\n",
    "        self.user_id = np.random.choice(self.valid_users, size = 1).item()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _convert_action_score_item(self, action_score):\n",
    "        items_ids = np.array(range(self.items_num))\n",
    "        \n",
    "        items_ids = np.setdiff1d(items_ids, self.recommended_items)\n",
    "        \n",
    "        items_ebs = self.embedding_network.get_layer('movie_embedding')(items_ids)\n",
    "#         action_score = tf.transpose(action_score, perm=(1,0))\n",
    "        action_score = tf.convert_to_tensor(np.expand_dims(action_score, 1))\n",
    "        \n",
    "        item_idx = np.argmax(tf.keras.backend.dot(items_ebs, action_score))\n",
    "        \n",
    "        recommendation_item = int(items_ids[item_idx])\n",
    "        \n",
    "        return recommendation_item\n",
    "        \n",
    "    \n",
    "    def _reset(self):        \n",
    "        self.step_count = 1\n",
    "        self.user_id = np.random.choice(self.valid_users, size = 1).item()\n",
    "        \n",
    "        self.user_df = self.ratings_df.loc[self.ratings_df[\"UserID\"] == self.user_id]\n",
    "#         self.movie_rate_dict = defaultdict(lambda: -0.5, zip(self.user_df[\"MovieID\"], self.user_df[\"Rating\"]))\n",
    "        self.state_items_ids = self.user_df.loc[self.user_df[\"Rating\"] >= 4, \"MovieID\"].head(self.state_size).values\n",
    "        self.user_items = self.user_df[\"MovieID\"].values\n",
    "        \n",
    "        self.recommended_items = self.state_items_ids.copy()\n",
    "        \n",
    "\n",
    "        self.user_eb = self.embedding_network.get_layer('user_embedding')(np.array(self.user_id))\n",
    "        state_items_eb = self.embedding_network.get_layer('movie_embedding')(np.array(self.state_items_ids))\n",
    "        \n",
    "        \n",
    "        self.srm_ave = DRRAveStateRepresentation(self.embedding_dim)\n",
    "        self._state = np.array(self.srm_ave([np.expand_dims(self.user_eb, axis=0), np.expand_dims(state_items_eb, axis=0)])[0], dtype = np.float32)\n",
    "        \n",
    "#         for x in self.recommended_items:\n",
    "#             self.movie_rate_dict[x] = -0.5\n",
    "            \n",
    "        self._episode_ended = False\n",
    "        \n",
    "        return ts.restart(self._state)\n",
    "        \n",
    "        \n",
    "    def _generate_valid_user(self):\n",
    "        temp = self.ratings_df.loc[ratings_df[\"Rating\"] >= 4].groupby([\"UserID\"])[\"Rating\"].count()\n",
    "        valid_users = temp.loc[temp >= self.state_size].index\n",
    "        \n",
    "        return valid_users\n",
    "    \n",
    "    def _step(self, action):\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return self.reset()    \n",
    "        \n",
    "        recommendation_item = self._convert_action_score_item(action)\n",
    "        self.recommendation_item = recommendation_item\n",
    "        \n",
    "        if recommendation_item in self.user_items:\n",
    "            if recommendation_item not in self.recommended_items:\n",
    "                rate = self.user_df.loc[self.user_df[\"MovieID\"] == recommendation_item, \"Rating\"].values[0]\n",
    "                reward = (rate-3)/2\n",
    "                if reward > 0:\n",
    "                    self.state_items_ids = np.append(self.state_items_ids[1:], values = recommendation_item)\n",
    "                    state_items_eb = self.embedding_network.get_layer('movie_embedding')(np.array(self.state_items_ids))\n",
    "                    self._state = np.array(self.srm_ave([np.expand_dims(self.user_eb, axis=0), np.expand_dims(state_items_eb, axis=0)])[0], dtype = np.float32)\n",
    "            else:\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = -0.5\n",
    "        \n",
    "        self.recommended_items = np.unique(np.append(self.recommended_items, recommendation_item))\n",
    "        \n",
    "        \n",
    "#         if len(self.recommended_items) >= 20:\n",
    "#             self._episode_ended = True\n",
    "        if self.step_count == self.max_step or len(np.setdiff1d(self.user_items, self.recommendation_item)) == 0:\n",
    "            self._episode_ended = True\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return ts.termination(np.array(self._state), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array(self._state), reward, discount = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "isolated-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_py = RS_Env(ratings_df, embedding_dim = 100, state_size = 10, embedding_network = embedding_network)\n",
    "eval_env_py = RS_Env(ratings_df, embedding_dim = 100, state_size = 10, embedding_network = embedding_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quantitative-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env_tf = tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "# eval_env_tf = tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-appraisal",
   "metadata": {},
   "source": [
    "### Actor Networktrain_env_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "paperback-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "latin-headline",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'tf.float32' as a data type\n  In call to configurable 'ActorNetwork' (<class 'tf_agents.agents.ddpg.actor_network.ActorNetwork'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eca1fceb5831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m actor_net  = tf_agents.agents.ddpg.actor_network.ActorNetwork(input_tensor_spec = train_env_py.observation_spec(), \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                               \u001b[0moutput_tensor_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# --> Only float actions are supported by this network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                               \u001b[0mfc_layer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                               \u001b[0mactivation_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                               name = \"ActorNetwork\")\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m_capture_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;31m# Avoid auto tracking which prevents keras from tracking layers that are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;31m# passed as kwargs to the Network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tf_agents/agents/ddpg/actor_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_tensor_spec, output_tensor_spec, fc_layer_params, dropout_layer_params, conv_layer_params, activation_fn, kernel_initializer, last_kernel_initializer, name)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_action_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_action_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_action_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Only float actions are supported by this network.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'tf.float32' as a data type\n  In call to configurable 'ActorNetwork' (<class 'tf_agents.agents.ddpg.actor_network.ActorNetwork'>)"
     ]
    }
   ],
   "source": [
    "actor_net  = tf_agents.agents.ddpg.actor_network.ActorNetwork(input_tensor_spec = train_env_py.observation_spec(), \n",
    "                                                              output_tensor_spec = train_env_py.action_spec(),\n",
    "                                                              fc_layer_params=[128, 128],\n",
    "                                                              activation_fn = tf.nn.relu,\n",
    "                                                              name = \"ActorNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "searching-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ids = np.array(range(items_num))\n",
    "movie_embedding = embedding_network.get_layer('movie_embedding')(items_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "massive-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_net = ActorNetwork(tf_rs_env.observation_spec(), tf_rs_env.action_spec(), embedding_dim = EMBEDDING_DIM, hidden_dim = 128, items_num = items_num, movie_embedding = movie_embedding, name = \"ActorNetwork2323\")\n",
    "\n",
    "# target_actor_net = ActorNetwork(tf_rs_env.observation_spec(), tf_rs_env.action_spec(), embedding_dim = EMBEDDING_DIM, hidden_dim = 128, items_num = items_num, movie_embedding = movie_embedding, name = \"TargetActorNetwork2323\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-allergy",
   "metadata": {},
   "source": [
    "### Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "successful-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = tf_agents.agents.ddpg.critic_network.CriticNetwork(\n",
    "    input_tensor_spec = (train_env_py.observation_spec(), train_env_py.action_spec()),\n",
    "    observation_fc_layer_params = [100],\n",
    "    joint_fc_layer_params = [128, 128],\n",
    "    activation_fn = tf.nn.relu,\n",
    "    output_activation_fn = tf.nn.relu,\n",
    "    name='CriticNetwork'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "outdoor-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTOR_LEARNIG_RATE = 0.001\n",
    "# CRITIC_LEARNIG_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-spotlight",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "given-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import train_utils\n",
    "train_step = train_utils.create_train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "negative-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ddpg_agent = tf_agents.agents.DdpgAgent(time_step_spec = train_env_tf.time_step_spec(),\n",
    "                                           action_spec = train_env_tf.action_spec(),\n",
    "                                           actor_network = actor_net,\n",
    "                                           critic_network = critic_net,\n",
    "                                           actor_optimizer = tf.keras.optimizers.Adam(learning_rate=ACTOR_LEARNIG_RATE),\n",
    "                                           critic_optimizer = tf.keras.optimizers.Adam(learning_rate=CRITIC_LEARNIG_RATE),\n",
    "#                                            target_actor_network = target_actor_net,\n",
    "#                                            target_critic_network = target_critic_net,\n",
    "                                           target_update_tau = 0.001,\n",
    "                                           target_update_period = 1,\n",
    "                                           gamma = 0.9,\n",
    "                                           ou_stddev=0.5,\n",
    "                                           ou_damping=0.15,\n",
    "                                           train_step_counter = train_step\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complicated-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ddpg_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "boxed-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY_BUFFER_MAX_LENGTH = 50000\n",
    "# NUM_EPISODE = 10000\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "south-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.ou_noise_policy.OUNoisePolicy object at 0x7f9083aa67f0>\". Calling saved_model.distribution() will raise the following assertion error: Distributions are not implemented yet.\n"
     ]
    }
   ],
   "source": [
    "my_policy = tf_ddpg_agent.collect_policy\n",
    "saver = PolicySaver(my_policy, batch_size = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-midnight",
   "metadata": {},
   "source": [
    "### Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "removed-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EVAL_EPISODES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "constant-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = environment.step(action_step.action)\n",
    "        episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "split-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(eval_env_tf, tf_ddpg_agent.policy, NUM_EVAL_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-printer",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "natural-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLAY_BUFFER_MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "grateful-flash",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec = tf_ddpg_agent.collect_data_spec,\n",
    "                                                               batch_size = train_env_tf.batch_size,\n",
    "                                                               max_length = REPLAY_BUFFER_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "consistent-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(num_parallel_calls=3, \n",
    "                                   sample_batch_size = BATCH_SIZE, \n",
    "                                   num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-fence",
   "metadata": {},
   "source": [
    "### Reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "verified-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "rolled-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "seeing-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pacific-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'PER_table'\n",
    "table = reverb.Table(table_name,               \n",
    "                     max_size=REPLAY_BUFFER_MAX_LENGTH,\n",
    "                     sampler=reverb.selectors.Prioritized(0.8),\n",
    "                     remover=reverb.selectors.Fifo(),\n",
    "                     rate_limiter=reverb.rate_limiters.MinSize(100))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "accepting-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(tf_ddpg_agent.collect_data_spec,\n",
    "                                                        sequence_length=2,\n",
    "                                                        table_name=table_name,\n",
    "                                                        local_server=reverb_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "innovative-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=BATCH_SIZE, num_steps=2).prefetch(4)\n",
    "# experience_dataset_fn = lambda: dataset\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-theory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "congressional-literature",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "military-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-candy",
   "metadata": {},
   "source": [
    "##### agent.policy : 평가 및 배포에 사용되는 기본 정책 (Noise X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "introductory-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_ddpg_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-george",
   "metadata": {},
   "source": [
    "##### agent.collect_policy : 데이터 수집에 사용되는 정책 (Noise O) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "directed-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_ddpg_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-shade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lined-commodity",
   "metadata": {},
   "source": [
    "### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "painted-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "verified-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(reverb_replay.py_client,\n",
    "                                                       table_name,\n",
    "                                                       sequence_length=2,\n",
    "                                                       stride_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "gross-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "utility-joint",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Actor doesn't support TFEnvironments yet.\n  In call to configurable 'Actor' (<class 'tf_agents.train.actor.Actor'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-91ddd140dd4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_step_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnvironmentSteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m collect_actor = actor.Actor(train_env_tf,\n\u001b[0m\u001b[1;32m      3\u001b[0m                             \u001b[0mcollect_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0msteps_per_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tf_agents/train/actor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, policy, train_step, steps_per_run, episodes_per_run, observers, metrics, reference_metrics, summary_dir, summary_interval, name)\u001b[0m\n\u001b[1;32m    116\u001b[0m           max_episodes=episodes_per_run)\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFEnvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actor doesn't support TFEnvironments yet.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown environment type.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Actor doesn't support TFEnvironments yet.\n  In call to configurable 'Actor' (<class 'tf_agents.train.actor.Actor'>)"
     ]
    }
   ],
   "source": [
    "# actor doesn't support TFEnvironments yet.?????????????????????????????????????????????????????????????\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(train_env_tf,\n",
    "                            collect_policy,\n",
    "                            train_step,\n",
    "                            steps_per_run=1,\n",
    "                            metrics=actor.collect_metrics(10),\n",
    "                            summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "                            observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-farmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-torture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-interference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-analysis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "authorized-branch",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "annoying-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "    episode_counter = 0\n",
    "    traj_counter = 0\n",
    "    environment.reset()\n",
    "#     print(episode_counter)\n",
    "    while episode_counter < num_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "        traj_counter += 1\n",
    "        # Add trajectory to the replay buffer\n",
    "        reverb_replay.add_batch(traj)           \n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1\n",
    "    return traj_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "focused-playback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ddpg_agent.train = common.function(tf_ddpg_agent.train)\n",
    "tf_ddpg_agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "union-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(eval_env_tf, tf_ddpg_agent.policy, NUM_EVAL_EPISODES)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-happiness",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(NUM_EPISODE)):\n",
    "    collect_episode(train_env_tf, tf_ddpg_agent.collect_policy, 1)\n",
    "    \n",
    "#     experience = replay_buffer.as_dataset()\n",
    "    experience, unused_info = next(iterator)\n",
    "#     experience = replay_buffer.gather_all()\n",
    "    train_loss = tf_ddpg_agent.train(experience)\n",
    "#     replay_buffer.clear()\n",
    "\n",
    "    step = tf_ddpg_agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env_tf, tf_ddpg_agent.policy, NUM_EVAL_EPISODES)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-talent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-chosen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-sheffield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4796,
   "id": "sound-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.compat.v1.train.get_or_create_global_step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
